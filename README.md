# ğŸš€ Big Data Cryptocurrency Streaming Platform

**Enterprise-Grade Real-Time Analytics & ML Pipeline for Cryptocurrency Market Analysis**

![Architecture](https://img.shields.io/badge/Architecture-Streaming%20First-blue)
![Status](https://img.shields.io/badge/Status-Production%20Ready-green)
![License](https://img.shields.io/badge/License-MIT-blue)

## ğŸ“Š Platform Overview

A **production-ready, scalable streaming platform** designed for real-time cryptocurrency market analysis, anomaly detection, and predictive analytics. Built with enterprise technologies: Apache Kafka, Spark Structured Streaming, Hadoop HDFS, and MLflow.

### Key Capabilities

- âœ… **Real-time Data Ingestion**: CoinGecko API â†’ Kafka (10+ cryptocurrencies, 60-second intervals)
- âœ… **Stream Processing**: Spark Structured Streaming with windowing and watermarking
- âœ… **Feature Engineering**: Moving averages, volatility, volume analysis (1/5/15-minute windows)
- âœ… **Anomaly Detection**: Isolation Forest + Z-score, pump-and-dump detection
- âœ… **ML Pipeline**: Random Forest classification, Logistic Regression trending
- âœ… **Data Lake**: HDFS Parquet storage with date/coin partitioning
- âœ… **ML Experiment Tracking**: MLflow with model versioning (Staging â†’ Production)
- âœ… **Real-time Alerts**: Kafka-based event streaming for anomalies
- âœ… **Monitoring**: Kafka UI, Spark UI, MLflow Dashboard

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CRYPTO STREAMING PLATFORM                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

           CoinGecko API
                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Kafka Producer       â”‚  (Python)
    â”‚  - Fetch 10+ coins    â”‚
    â”‚  - 60s interval       â”‚
    â”‚  - JSON serialization â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Kafka Broker         â”‚  (cluster: 1 broker)
    â”‚  Topics:              â”‚
    â”‚  - crypto_raw         â”‚
    â”‚  - crypto_clean       â”‚
    â”‚  - crypto_features    â”‚
    â”‚  - crypto_alerts      â”‚
    â”‚  - crypto_predictions â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Spark Structured Streaming              â”‚
    â”‚  - Watermarking (10 min delay)            â”‚
    â”‚  - Windows (1/5/15 min)                   â”‚
    â”‚  - Feature engineering                    â”‚
    â”‚  - Anomaly detection (Z-score)            â”‚
    â”‚  - ML inference                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  HDFS Data Lake (Parquet)                               â”‚
    â”‚  /data/crypto/                                          â”‚
    â”‚  â”œâ”€â”€ raw/        [parquet partitioned by symbol]        â”‚
    â”‚  â”œâ”€â”€ clean/      [cleaned & validated]                  â”‚
    â”‚  â”œâ”€â”€ features/   [engineered features]                  â”‚
    â”‚  â”œâ”€â”€ predictions/[ML model outputs]                     â”‚
    â”‚  â””â”€â”€ alerts/     [anomalies detected]                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MLflow (ML Experiment Tracking)         â”‚
    â”‚  - Model versioning                      â”‚
    â”‚  - Metrics & parameters logging          â”‚
    â”‚  - Model registry (Staging/Production)   â”‚
    â”‚  - Artifact storage                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Real-time Alerts â†’ Kafka crypto_alerts  â”‚
    â”‚  (Anomalies, pump-and-dump events)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Project Structure

```
crypto-bigdata-platform/
â”œâ”€â”€ kafka/                          # Kafka Producer
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ producer.py                # Main producer (CoinGecko â†’ Kafka)
â”‚   â”œâ”€â”€ config.py                  # Configuration management
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ spark/                          # Spark Streaming & ML
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ streaming_job.py            # Main streaming pipeline
â”‚   â”œâ”€â”€ train_model.py              # Model training (offline)
â”‚   â”œâ”€â”€ spark_config.py             # Spark configuration
â”‚   â”œâ”€â”€ utils.py                    # Helper utilities
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ mlflow/                         # MLflow tracking
â”‚   â”œâ”€â”€ mlruns/                     # Experiment runs
â”‚   â””â”€â”€ artifacts/                  # Model artifacts
â”‚
â”œâ”€â”€ hdfs/                           # HDFS Data Lake
â”‚   â””â”€â”€ data/crypto/                # Parquet storage
â”‚
â”œâ”€â”€ docker-compose.yml              # Complete orchestration
â”œâ”€â”€ requirements.txt                # Python dependencies
â””â”€â”€ README.md                       # This file
```

---

## ğŸ› ï¸ Technologies

| Component | Technology | Version |
|-----------|-----------|---------|
| Message Broker | Apache Kafka | 7.5.1 |
| Stream Processing | Apache Spark | 3.5.0 |
| Distributed Storage | Hadoop HDFS | 3.3.4 |
| ML Experiment Tracking | MLflow | 2.10.0 |
| Coordination | Zookeeper | (latest) |
| Monitoring | Kafka UI | (latest) |
| Language | Python | 3.10+ |

---

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose installed
- 8GB+ RAM available
- Python 3.10+ (for local development)

### 1. Clone & Setup

```bash
# Navigate to project directory
cd crypto-bigdata-platform

# Create directories
mkdir -p mlflow/{mlruns,artifacts} hdfs
```

### 2. Start the Platform

```bash
# Build and start all services
docker-compose up -d

# Verify all services are running
docker-compose ps

# Expected output:
# kafka             Running
# zookeeper         Running
# kafka-ui          Running
# namenode          Running
# datanode          Running
# spark-master      Running
# spark-worker-1    Running
# mlflow            Running
# crypto-producer   Running
# spark-streaming   Running
```

### 3. Monitor & Access Services

| Service | URL | Purpose |
|---------|-----|---------|
| **Kafka UI** | http://localhost:8080 | Kafka topic monitoring |
| **Spark Master** | http://localhost:8081 | Spark cluster dashboard |
| **HDFS Namenode** | http://localhost:9870 | HDFS file browser |
| **MLflow** | http://localhost:5000 | Experiment tracking |

### 4. Verify Data Flow

```bash
# Check Kafka topics
docker-compose exec kafka kafka-topics --list --bootstrap-server localhost:9092

# Expected topics:
# crypto_raw
# crypto_clean
# crypto_features
# crypto_alerts
# crypto_predictions

# Read messages from crypto_raw topic
docker-compose exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic crypto_raw \
  --from-beginning \
  --max-messages 5
```

### 5. Train ML Models

```bash
# Submit training job to Spark
docker-compose exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --total-executor-cores 4 \
  --executor-memory 2g \
  --driver-memory 2g \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  /app/spark/train_model.py
```

### 6. View MLflow Experiments

Navigate to **http://localhost:5000** to:
- View experiment runs
- Compare model metrics (AUC, loss, etc.)
- Register models for production
- Track artifacts

---

## ğŸ“Š Data Pipeline Details

### 1. Kafka Producer (CoinGecko â†’ Kafka)

**Location**: [kafka/producer.py](kafka/producer.py)

**Functionality**:
- Fetches data for 10+ cryptocurrencies from CoinGecko API
- 60-second polling interval (configurable)
- JSON serialization
- Retry logic with exponential backoff
- Graceful error handling

**Environment Variables**:
```bash
KAFKA_BOOTSTRAP_SERVERS=kafka:29092
KAFKA_PRODUCER_TOPIC=crypto_raw
COINGECKO_API_POLLING_INTERVAL=60  # seconds
```

**Message Format**:
```json
{
  "id": "bitcoin",
  "symbol": "BITCOIN",
  "price": 43250.50,
  "market_cap": 850000000000,
  "volume": 28000000000,
  "price_change_24h": 2.5,
  "last_updated": 1704062400,
  "timestamp": "2024-01-01T12:00:00Z",
  "source": "coingecko"
}
```

---

### 2. Spark Structured Streaming

**Location**: [spark/streaming_job.py](spark/streaming_job.py)

**Key Operations**:

#### Data Cleaning
- Filter null/invalid prices
- Remove duplicates
- Type validation
- Timestamp normalization

#### Feature Engineering (Windowed)
- **1-minute MA**: `avg(price)` over 1-min window
- **5-minute MA**: `avg(price)` over 5-min window
- **15-minute MA**: `avg(price)` over 15-min window
- **Volatility**: `stddev(price)` over 1-min window
- **Volume Change**: `(current_vol - prev_vol) / prev_vol * 100%`

#### Anomaly Detection
- **Z-score Anomaly**: `|price - mean| / stddev > 3.0`
- **Volume Spike**: `volume_change_pct > 200%`
- **Combined Flag**: `is_anomaly = is_price_anomaly OR is_volume_anomaly`

#### Output Destinations
- **Clean Data** â†’ HDFS `/data/crypto/clean/` (Parquet)
- **Features** â†’ HDFS `/data/crypto/features/` (Parquet)
- **Alerts** â†’ Kafka `crypto_alerts` topic (JSON)

---

### 3. ML Models

**Location**: [spark/train_model.py](spark/train_model.py)

#### Model 1: Anomaly Detection
- **Algorithm**: Random Forest (100 trees)
- **Features**: price, volume, MAs, volatility, volume_change
- **Labels**: is_anomaly (binary)
- **Metrics**: AUC-ROC
- **Registry Name**: `CryptoAnomalyDetection`

#### Model 2: Trend Prediction
- **Algorithm**: Logistic Regression (with feature scaling)
- **Features**: price, MAs (1/5/15min), volatility, volume_change
- **Labels**: UP (1) / DOWN (0)
- **Metrics**: AUC-ROC
- **Registry Name**: `CryptoTrendPrediction`

**Training Data**: 80/20 split, sampled from HDFS features

---

### 4. HDFS Data Lake Structure

```
hdfs://namenode:8020/data/crypto/
â”‚
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ symbol=bitcoin/
â”‚       â”œâ”€â”€ 2024-01-01/
â”‚       â”‚   â””â”€â”€ part-00000.parquet
â”‚       â””â”€â”€ 2024-01-02/
â”‚           â””â”€â”€ part-00000.parquet
â”‚
â”œâ”€â”€ clean/
â”‚   â””â”€â”€ symbol=ethereum/
â”‚       â”œâ”€â”€ 2024-01-01/
â”‚       â””â”€â”€ 2024-01-02/
â”‚
â”œâ”€â”€ features/
â”‚   â””â”€â”€ symbol=cardano/
â”‚       â”œâ”€â”€ 2024-01-01/
â”‚       â””â”€â”€ 2024-01-02/
â”‚
â”œâ”€â”€ predictions/
â”‚   â””â”€â”€ symbol=solana/
â”‚       â””â”€â”€ 2024-01-01/
â”‚
â””â”€â”€ alerts/
    â”œâ”€â”€ 2024-01-01/
    â”‚   â””â”€â”€ pump_dump_alerts.parquet
    â””â”€â”€ 2024-01-02/
        â””â”€â”€ pump_dump_alerts.parquet
```

**Partitioning Strategy**: `symbol` (coin) + `date` (daily) for efficient querying

---

### 5. Real-time Alerts (Kafka)

**Topic**: `crypto_alerts`

**Alert Format**:
```json
{
  "symbol": "BITCOIN",
  "price": 43250.50,
  "anomaly_score": 4.2,
  "is_price_anomaly": true,
  "is_volume_anomaly": false,
  "event_timestamp": "2024-01-01T12:05:30Z",
  "event_type": "ALERT"
}
```

---

## ğŸ¤– Machine Learning Workflow

### Training Phase

```bash
# 1. Run training job (every 24 hours recommended)
docker-compose exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  /app/spark/train_model.py

# 2. Models are logged to MLflow
# 3. View results at http://localhost:5000
# 4. Promote best model to Production registry
```

### Serving Phase (Real-time Inference)

- Running Spark job loads production models from MLflow
- Applies models to streaming features
- Writes predictions back to Kafka/HDFS

---

## ğŸ“ˆ Monitoring & Observability

### Kafka UI (Topic Monitoring)

- **URL**: http://localhost:8080
- **Features**:
  - Real-time message viewing
  - Lag monitoring
  - Topic stats
  - Consumer groups

### Spark UI (Job Monitoring)

- **URL**: http://localhost:8081
- **Features**:
  - Active jobs & stages
  - Executor metrics
  - Task distribution
  - Memory usage

### MLflow Dashboard (Experiment Tracking)

- **URL**: http://localhost:5000
- **Features**:
  - Experiment comparisons
  - Metric visualization
  - Model versioning
  - Artifact browsing

### HDFS Web UI (Data Lake Monitoring)

- **URL**: http://localhost:9870
- **Features**:
  - File browser
  - Storage usage
  - Datanode status
  - Block information

---

## ğŸ§ª Testing & Validation

### 1. Producer Test

```bash
# Check producer logs
docker-compose logs crypto-producer -f

# Expected output:
# Iteration 1 (2024-01-01T12:00:00Z)
# ğŸ“¤ Sent BITCOIN: $43250.5 to crypto_raw
# ğŸ“¤ Sent ETHEREUM: $2280.3 to crypto_raw
```

### 2. Streaming Job Test

```bash
# Check streaming logs
docker-compose logs spark-streaming -f

# Monitor with Kafka UI: http://localhost:8080
```

### 3. Anomaly Detection Test

```bash
# Read alerts
docker-compose exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic crypto_alerts \
  --from-beginning
```

### 4. Data Lake Verification

```bash
# Browse HDFS files
docker-compose exec namenode hadoop fs -ls /data/crypto/clean/

# Count records
docker-compose exec namenode hadoop fs -count /data/crypto/clean/
```

---

## ğŸ”§ Configuration

### Kafka Producer (`kafka/config.py`)

```python
COINGECKO_COINS = ['bitcoin', 'ethereum', 'cardano', ...]  # Add/remove coins
POLLING_INTERVAL = 60  # seconds
```

### Spark Streaming (`spark/spark_config.py`)

```python
BATCH_INTERVAL = 30  # seconds
WATERMARK_DELAY = "10 minutes"  # For stateful operations
ANOMALY_THRESHOLDS = {
    'z_score': 3.0,       # Standard deviations
    'volume_spike': 2.0,  # 2x normal volume
    'price_spike': 0.10   # 10% change
}
```

### Environment Variables

```bash
# Kafka
KAFKA_BOOTSTRAP_SERVERS=kafka:29092
KAFKA_PRODUCER_TOPIC=crypto_raw

# Spark
SPARK_MASTER_URL=spark://spark-master:7077

# HDFS
HDFS_NAMENODE=hdfs://namenode:8020

# MLflow
MLFLOW_TRACKING_URI=http://mlflow:5000

# Logging
LOG_LEVEL=INFO
```

---

## ğŸš¨ Troubleshooting

### Issue: Kafka Producer can't connect

```bash
# Check Kafka broker status
docker-compose logs kafka

# Verify broker is listening
docker-compose exec kafka nc -zv kafka 29092
```

### Issue: Spark streaming job crashes

```bash
# View Spark logs
docker-compose logs spark-streaming -f

# Check HDFS connectivity
docker-compose exec spark-master hadoop fs -ls /data/crypto/
```

### Issue: HDFS namenode won't start

```bash
# Reset HDFS state (if needed)
docker-compose down -v
docker-compose up namenode datanode -d

# Wait for namenode to be healthy
docker-compose ps namenode
```

### Issue: MLflow models not appearing

```bash
# Verify MLflow server is running
curl http://localhost:5000/api/2.0/mlflow/experiments/list

# Check model registry
curl http://localhost:5000/api/2.0/mlflow/registered-models/list
```

---

## ğŸ“ API Examples

### Kafka Consumer (Python)

```python
from kafka import KafkaConsumer
import json

consumer = KafkaConsumer(
    'crypto_raw',
    bootstrap_servers=['localhost:9093'],
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)

for message in consumer:
    print(f"Price: {message['value']['price']}")
```

### HDFS Query (PySpark)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Query").getOrCreate()

df = spark.read.parquet("hdfs://namenode:8020/data/crypto/features/")
df.filter(df.symbol == "BITCOIN").show()
```

### MLflow Model Usage

```python
import mlflow

client = mlflow.tracking.MlflowClient("http://localhost:5000")
model_version = client.get_model_version("CryptoAnomalyDetection", "1")
model_uri = f"models:/CryptoAnomalyDetection/production"

model = mlflow.sklearn.load_model(model_uri)
predictions = model.predict(features)
```

---

## ğŸ“Š Performance Metrics

### Benchmarks (Single-threaded)

| Component | Throughput | Latency |
|-----------|-----------|---------|
| Producer | 10 msgs/sec (per coin Ã— 1-min interval) | <100ms per fetch |
| Streaming | 1000+ events/sec (Spark) | <5s end-to-end |
| ML Inference | 500+ predictions/sec (per model) | <50ms per batch |
| HDFS Write | 50MB+/sec | Append-only |

### Resource Usage

- **Kafka**: 256MB RAM
- **Spark Master**: 2GB RAM
- **Spark Worker**: 2GB RAM (per worker)
- **HDFS**: 1GB RAM (namenode) + 1GB (datanode)
- **MLflow**: 512MB RAM
- **Total**: ~8-10GB for full stack

---

## ğŸ” Security Considerations

### Production Recommendations

1. **Authentication**
   - Enable Kafka SASL/SSL
   - Restrict Spark/HDFS access with Kerberos

2. **Encryption**
   - Enable SSL for Kafka, HDFS, MLflow
   - Encrypt data at rest

3. **Access Control**
   - Use role-based access (RBAC)
   - Audit all data lake access

4. **Monitoring**
   - Setup alerts for data quality issues
   - Monitor resource usage
   - Track model drift

---

## ğŸ“š References

- [Apache Kafka Documentation](https://kafka.apache.org/documentation/)
- [Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Hadoop HDFS](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html)
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [CoinGecko API](https://www.coingecko.com/en/api)

---

## ğŸ“„ License

MIT License - See LICENSE file

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create feature branch
3. Submit pull request

---

## ğŸ“ Support

For issues, questions, or suggestions:
- Open GitHub issue
- Check troubleshooting section
- Review logs: `docker-compose logs [service]`

---

**Built with â¤ï¸ for Big Data & ML Excellence**
