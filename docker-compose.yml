services:
  # Zookeeper for Kafka coordination
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.1
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9093:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - kafka_data:/var/lib/kafka/data

  # Kafka UI (optional - for monitoring)
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092

  # REAL Hadoop Namenode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    ports:
      - "9870:9870"
      - "9000:9000"
      - "8020:8020"
    volumes:
      - namenode:/hadoop/dfs/name
      - ./hdfs:/hdfs_scripts  # <--- MOUNT YOUR SCRIPT FOLDER HERE
    env_file:
      - ./hadoop.env

  # REAL Hadoop Datanode
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    volumes:
      - datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    depends_on:
      - namenode

  # Spark Master
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-master
    ports:
      - "8081:8080"
      - "7077:7077"
      - "4040:4040"
    working_dir: /app
    volumes:
      - ./spark:/app
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      HDFS_NAMENODE: hdfs://namenode:8020
      MLFLOW_TRACKING_URI: http://mlflow:5000
    depends_on:
      - kafka
      - namenode
      - mlflow
    command: python -u /app/streaming_job.py

  # Spark Worker
  spark-worker-1:
    image: python:3.9-slim
    container_name: spark-worker-1
    depends_on:
      - spark-master
    ports:
      - "8082:8080"
      - "7078:7078"
    working_dir: /app
    volumes:
      - ./spark:/app
    command: sleep 3600

  # MLflow Server
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.10.0
    container_name: mlflow
    ports:
      - "5000:5000"
    volumes:
      - ./mlflow/mlruns:/mlruns
      - ./mlflow/artifacts:/artifacts
    command: mlflow server --host 0.0.0.0 --port 5000 --backend-store-uri file:///mlruns --default-artifact-root /artifacts

  # Kafka Producer
  crypto-producer:
    build:
      context: ./kafka
      dockerfile: Dockerfile
    container_name: crypto-producer
    depends_on:
      - kafka
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_PRODUCER_TOPIC: crypto_raw
      COINGECKO_API_POLLING_INTERVAL: 60
    volumes:
      - ./kafka:/app
    restart: unless-stopped

  # Spark Streaming Job
  spark-streaming:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-streaming
    depends_on:
      - spark-master
      - kafka
      - namenode
      - mlflow
    environment:
      SPARK_MASTER_URL: spark://spark-master:7077
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      HDFS_NAMENODE: hdfs://namenode:8020
      MLFLOW_TRACKING_URI: http://mlflow:5000
    volumes:
      - ./spark:/app
      - ./hdfs:/data
    restart: unless-stopped

  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: dashboard
    ports:
      - "8501:8501"
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    volumes:
      - ./dashboard:/app
      - ./hdfs:/data  # <--- THIS IS KEY: Access HDFS files locally
    depends_on:
      - kafka
      - namenode
      - mlflow

volumes:
  zookeeper_data:
  zookeeper_logs:
  kafka_data:
  namenode:
  datanode:
  spark_master_data:
  spark_worker_1_data:

networks:
  default:
    name: crypto_platform_network
